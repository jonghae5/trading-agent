import json
import re
import os
from datetime import datetime, timedelta
from functools import lru_cache
from typing import Any, Dict, List
from urllib.parse import quote
import requests


class NaverSearchClient:
    def __init__(self):
        self.client_id = os.getenv("NAVER_CLIENT_ID", "I6QgY4M3jZJtB6SV6jU5")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET", "wMwj8EOwkS") 
        self.base_url = os.getenv("NAVER_SEARCH_BASE_URL", "https://openapi.naver.com/v1/search")
        self.timeout_seconds = 30
        print(f"[NaverNews] ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - Client ID: {self.client_id[:10]}...")
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # <b> íƒœê·¸ ì œê±°
        text = re.sub(r'<b>', '', text)
        text = re.sub(r'</b>', '', text)
        
        # ë‹¤ë¥¸ HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # HTML ì—”í„°í‹° ë””ì½”ë”©
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&amp;', '&')
        text = text.replace('&quot;', '"')
        text = text.replace('&#39;', "'")
        
        return text.strip()
    
    def _format_date(self, pub_date: str) -> str:
        """Naver API ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # "Mon, 26 Sep 2016 07:50:00 +0900" -> "2016-09-26"
            dt = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except:
            try:
                # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                dt = datetime.strptime(pub_date[:19], "%a, %d %b %Y %H:%M:%S")
                return dt.strftime("%Y-%m-%d")
            except:
                # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ
                return datetime.now().strftime("%Y-%m-%d")
    
    def _extract_source_from_url(self, url: str) -> str:
        """URLì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
        try:
            # ë„ë©”ì¸ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            domain_patterns = {
                'yonhapnews': 'ì—°í•©ë‰´ìŠ¤',
                'chosun': 'ì¡°ì„ ì¼ë³´',
                'joongang': 'ì¤‘ì•™ì¼ë³´',
                'donga': 'ë™ì•„ì¼ë³´',
                'hani': 'í•œê²¨ë ˆ',
                'khan': 'ê²½í–¥ì‹ ë¬¸',
                'segye': 'ì„¸ê³„ì¼ë³´',
                'munhwa': 'ë¬¸í™”ì¼ë³´',
                'seoul': 'ì„œìš¸ì‹ ë¬¸',
                'kookje': 'êµ­ì œì‹ ë¬¸',
                'busan': 'ë¶€ì‚°ì¼ë³´',
                'kwnews': 'ê´‘ì£¼ì¼ë³´',
                'etnews': 'ì „ìì‹ ë¬¸',
                'mk': 'ë§¤ì¼ê²½ì œ',
                'hankyung': 'í•œêµ­ê²½ì œ',
                'fnnews': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
                'newsis': 'ë‰´ì‹œìŠ¤',
                'news1': 'ë‰´ìŠ¤1',
                'yna': 'ì—°í•©ë‰´ìŠ¤',
                'naver': 'ë„¤ì´ë²„ë‰´ìŠ¤'
            }
            
            url_lower = url.lower()
            for pattern, source in domain_patterns.items():
                if pattern in url_lower:
                    return source
            
            # ë„ë©”ì¸ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
            import re
            match = re.search(r'://([^/]+)', url)
            if match:
                domain = match.group(1)
                domain = domain.replace('www.', '').replace('.com', '').replace('.co.kr', '')
                return domain.title()
            
            return "ì–¸ë¡ ì‚¬"
            
        except:
            return "ì–¸ë¡ ì‚¬"
    
    def search_news(self, query: str, display: int = 10, start: int = 1, sort: str = "date") -> Dict[str, Any]:
        """
        Naver News APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ì–´
            display: í‘œì‹œí•  ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10, ìµœëŒ€: 100)
            start: ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜ (ê¸°ë³¸ê°’: 1, ìµœëŒ€: 1000) 
            sort: ì •ë ¬ ë°©ë²• ("sim": ì •í™•ë„ìˆœ, "date": ë‚ ì§œìˆœ)
        """
        
        if not self.client_id or not self.client_secret:
            print(f"[NaverNews] âŒ API í‚¤ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ê²€ìƒ‰ì–´: {query}")
            return {
                "query": query,
                "total_count": 0,
                "news": [],
                "error": "Naver API credentials not configured"
            }
        
        # ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ê°œì„ 
        current_year = datetime.now().year
        
        # ê²€ìƒ‰ì–´ì— ìµœì‹  í‚¤ì›Œë“œ ì¶”ê°€ (sortê°€ dateì¼ ë•Œë§Œ)
        enhanced_query = query
            
        print(f"[NaverNews] ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘ - ì›ë³¸: '{query}' â†’ ê°œì„ : '{enhanced_query}' (ì •ë ¬: {sort})")
        encoded_query = enhanced_query
        url = f"{self.base_url}/news.json"
        
        params = {
            'query': encoded_query,
            'display': min(display, 100),  # ìµœëŒ€ 100ê°œ
            'start': min(start, 1000),     # ìµœëŒ€ 1000
            'sort': sort if sort in ['sim', 'date'] else 'date'  # ê¸°ë³¸ê°’ì„ date(ìµœì‹ ìˆœ)ë¡œ ë³€ê²½
        }
        
        headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret,
            'User-Agent': 'NaverSearchClient/1.0'
        }
        try:
            print(f"[NaverNews] ğŸ“¡ API í˜¸ì¶œ ì¤‘... URL: {url}")
            response = requests.get(url, params=params, headers=headers, timeout=self.timeout_seconds)
            
            if response.status_code == 200:
                data = response.json()
                total_count = data.get('total', 0)
                print(f"[NaverNews] âœ… API ì‘ë‹µ ì„±ê³µ - ì´ {total_count}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                return self._parse_naver_response(data, query, enhanced_query)
            else:
                print(f"[NaverNews] âŒ API ì˜¤ë¥˜ - ìƒíƒœì½”ë“œ: {response.status_code}")
                raise Exception(f"Naver API Error {response.status_code}: {response.text}")
                            
        except Exception as e:
            print(f"[NaverNews] âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return {
                "query": query,
                "total_count": 0,
                "news": [],
                "error": str(e)
            }
    
    def _calculate_relevance_score(self, title: str, query: str) -> int:
        """ì œëª©ê³¼ ì¿¼ë¦¬ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)"""
        title_lower = title.lower().strip()
        query_lower = query.lower().strip()
        
        score = 0
        print(f"[NaverNews] ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° - ì œëª©: '{title[:30]}...' vs ì¿¼ë¦¬: '{query}'")
        
        # 1. ì™„ì „ ì¿¼ë¦¬ ì¼ì¹˜ (ìµœê³  ì ìˆ˜)
        if query_lower in title_lower:
            score += 200  # ì™„ì „ ì¼ì¹˜ëŠ” ë§¤ìš° ë†’ì€ ì ìˆ˜
        
        # 2. ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ë³„ ìŠ¤ì½”ì–´ë§
        query_words = [word.strip() for word in query_lower.split() if word.strip()]
        title_words = [word.strip() for word in title_lower.split() if word.strip()]
        
        if not query_words:
            return score
        
        matched_words = 0
        word_scores = []
        
        for query_word in query_words:
            word_score = 0
            
            # 2-1. ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹˜
            if query_word in title_words:
                word_score += 50  # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹˜
                matched_words += 1
            
            # 2-2. ì œëª©ì— ì¿¼ë¦¬ ë‹¨ì–´ê°€ ë¶€ë¶„ì ìœ¼ë¡œ í¬í•¨
            elif query_word in title_lower:
                word_score += 30  # ë¶€ë¶„ í¬í•¨
                matched_words += 0.7
            
            # 2-3. ì œëª© ë‹¨ì–´ ì¤‘ì— ì¿¼ë¦¬ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ ìˆëŠ”ì§€
            else:
                for title_word in title_words:
                    if len(query_word) >= 2:
                        # ì¿¼ë¦¬ ë‹¨ì–´ê°€ ì œëª© ë‹¨ì–´ì— í¬í•¨
                        if query_word in title_word:
                            word_score += 20
                            matched_words += 0.5
                            break
                        # ì œëª© ë‹¨ì–´ê°€ ì¿¼ë¦¬ ë‹¨ì–´ì— í¬í•¨ (ì§§ì€ ë‹¨ì–´ì˜ ê²½ìš°)
                        elif title_word in query_word and len(title_word) >= 2:
                            word_score += 15
                            matched_words += 0.3
                            break
            
            # 2-4. ë‹¨ì–´ ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ (ê¸´ ë‹¨ì–´ì¼ìˆ˜ë¡ ì¤‘ìš”)
            if word_score > 0:
                length_bonus = min(len(query_word) * 2, 10)  # ìµœëŒ€ 10ì  ë³´ë„ˆìŠ¤
                word_score += length_bonus
            
            word_scores.append(word_score)
        
        # 3. ì „ì²´ ë‹¨ì–´ ë§¤ì¹˜ ë¹„ìœ¨ ê³„ì‚°
        total_words = len(query_words)
        match_ratio = matched_words / total_words if total_words > 0 else 0
        
        # 4. ì ìˆ˜ í•©ì‚°
        score += sum(word_scores)
        
        # 5. ë§¤ì¹˜ ë¹„ìœ¨ ë³´ë„ˆìŠ¤
        if match_ratio >= 1.0:  # ëª¨ë“  ë‹¨ì–´ê°€ ë§¤ì¹˜ë¨
            score += 100
        elif match_ratio >= 0.8:  # 80% ì´ìƒ ë§¤ì¹˜
            score += 50
        elif match_ratio >= 0.5:  # 50% ì´ìƒ ë§¤ì¹˜
            score += 25
        
        # 6. ì œëª©ì—ì„œ ì¿¼ë¦¬ ë‹¨ì–´ë“¤ì´ ì—°ì†ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš° ë³´ë„ˆìŠ¤
        if len(query_words) > 1:
            query_phrase = ' '.join(query_words)
            if query_phrase in title_lower:
                score += 80  # ì—°ì† êµ¬ë¬¸ ë§¤ì¹˜ ë³´ë„ˆìŠ¤
        
        # 7. ì œëª© ê¸¸ì´ ëŒ€ë¹„ ë§¤ì¹˜ ë¹„ìœ¨ (ì œëª©ì´ ì§§ê³  ë§¤ì¹˜ê°€ ë§ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        if title_words:
            title_relevance = (matched_words / len(title_words)) * 20
            score += int(title_relevance)
        
        print(f"[NaverNews] ğŸ“Š ìµœì¢… ê´€ë ¨ì„± ì ìˆ˜: {score}")
        return score

    def _parse_naver_response(self, data: Dict[str, Any], query: str, enhanced_query: str = None) -> Dict[str, Any]:
        """Naver API ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        news_items = []
        items = data.get('items', [])
        print(f"[NaverNews] ğŸ“° ë‰´ìŠ¤ íŒŒì‹± ì‹œì‘ - {len(items)}ê°œ ì›ë³¸ ì•„ì´í…œ")
        
        # ìµœê·¼ 30ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ í•„í„°ë§
        cutoff_date = datetime.now() - timedelta(days=30)
        
        for item in items:
            # HTML íƒœê·¸ ì •ë¦¬
            title = self._clean_html_tags(item.get('title', ''))
            description = self._clean_html_tags(item.get('description', ''))
            
            # ì›ë³¸ ë§í¬ ìš°ì„ , ì—†ìœ¼ë©´ ë„¤ì´ë²„ ë§í¬ ì‚¬ìš©
            url = item.get('originallink') or item.get('link', '')
            
            # ë‚ ì§œ í˜•ì‹ ë³€í™˜
            pub_date = self._format_date(item.get('pubDate', ''))
            
            # 30ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ í¬í•¨ (ë‚ ì§œ íŒŒì‹±ì´ ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                news_date = datetime.strptime(pub_date, "%Y-%m-%d")
                if news_date < cutoff_date:
                    continue  # 30ì¼ ì´ìƒ ëœ ë‰´ìŠ¤ëŠ” ì œì™¸
            except:
                pass  # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨
            
            # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            source = self._extract_source_from_url(url)
            
            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            relevance_score = self._calculate_relevance_score(title, query)
            
            news_item = {
                "title": title,
                "url": url,
                "source": source,
                "published_date": pub_date,
                "summary": description,
                "relevance_score": relevance_score  # ì •ë ¬ìš© ì ìˆ˜
            }
            
            news_items.append(news_item)
        
        print(f"[NaverNews] ğŸ”„ ìŠ¤ë§ˆíŠ¸ ì •ë ¬ ì‹œì‘ - {len(news_items)}ê°œ ì•„ì´í…œ")
        # ìŠ¤ë§ˆíŠ¸ ì •ë ¬: 1ìˆœìœ„ ê´€ë ¨ì„±, 2ìˆœìœ„ ë‚ ì§œ
        news_items.sort(key=lambda x: (
            -x["relevance_score"],  # ê´€ë ¨ì„± ì ìˆ˜ ë†’ì€ ìˆœ
            -self._date_to_timestamp(x["published_date"])  # ë‚ ì§œ ìµœì‹  ìˆœ
        ))
        
        # ì •ë ¬ í›„ ìƒìœ„ 3ê°œ ì•„ì´í…œì˜ ì ìˆ˜ ë¡œê¹…
        if news_items:
            print(f"[NaverNews] ğŸ† ìƒìœ„ 3ê°œ ë‰´ìŠ¤:")
            for i, item in enumerate(news_items[:3]):
                score = item.get("relevance_score", 0)
                title = item.get("title", "")[:50]
                print(f"[NaverNews]   {i+1}ìœ„. ì ìˆ˜ {score}: {title}...")
        
        # ì •ë ¬ í›„ relevance_score ì œê±° (ì‘ë‹µì—ì„œ ì œì™¸)
        for item in news_items:
            item.pop("relevance_score", None)
        
        result = {
            "query": query,
            "enhanced_query": enhanced_query or query,
            "total_count": data.get('total', 0),
            "start": data.get('start', 1),
            "display": data.get('display', len(news_items)),
            "filtered_count": len(news_items),  # ì‹¤ì œ í•„í„°ë§ëœ ë‰´ìŠ¤ ìˆ˜
            "news": news_items
        }
        
        print(f"[NaverNews] âœ… íŒŒì‹± ì™„ë£Œ - ì´ {result['total_count']}ê°œ ì¤‘ {result['filtered_count']}ê°œ í•„í„°ë§ë¨")
        return result
    
    def _date_to_timestamp(self, date_str: str) -> int:
        """ë‚ ì§œ ë¬¸ìì—´ì„ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜ (ì •ë ¬ìš©)"""
        try:
            dt = datetime.strptime(date_str, "%Y-%m-%d")
            return int(dt.timestamp())
        except:
            return 0  # íŒŒì‹± ì‹¤íŒ¨ì‹œ 0 (ê°€ì¥ ì˜¤ë˜ëœ ê²ƒìœ¼ë¡œ ì·¨ê¸‰)
    
    def search_general(self, query: str, display: int = 10) -> Dict[str, Any]:
        """ì¼ë°˜ ê²€ìƒ‰ (ë‰´ìŠ¤ ì™¸)"""
        if not self.client_id or not self.client_secret:
            return {
                "status": "error",
                "query": query,
                "results": [],
                "error": "Naver API credentials not configured"
            }
        
        # ì›¹ë¬¸ì„œ ê²€ìƒ‰ API ì‚¬ìš©
        encoded_query = quote(query)
        url = f"{self.base_url}/webkr.json"
        
        params = {
            'query': encoded_query,
            'display': min(display, 100),
            'start': 1
        }
        
        headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret,
            'User-Agent': 'NaverSearchClient/1.0'
        }
        
        try:
            response = requests.get(url, params=params, headers=headers, timeout=self.timeout_seconds)
            if response.status_code == 200:
                data = response.json()
                return self._parse_web_response(data, query)
            else:
                raise Exception(f"Naver API Error {response.status_code}: {response.text}")
                            
        except Exception as e:
            return {
                "status": "error",
                "query": query,
                "results": [],
                "error": str(e)
            }
    
    def _parse_web_response(self, data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """ì›¹ ê²€ìƒ‰ ì‘ë‹µ íŒŒì‹±"""
        
        results = []
        items = data.get('items', [])
        
        for item in items:
            title = self._clean_html_tags(item.get('title', ''))
            description = self._clean_html_tags(item.get('description', ''))
            url = item.get('link', '')
            
            results.append({
                "title": title,
                "url": url,
                "description": description
            })
        
        return {
            "status": "success",
            "query": query,
            "total_count": data.get('total', 0),
            "results": results
        }


@lru_cache
def get_naver_search_client():
    return NaverSearchClient()


def is_korea_stock(ticker: str):
    """í•œêµ­ ì£¼ì‹ í‹°ì»¤ì¸ì§€ í™•ì¸"""
    if ticker.isdigit() and len(ticker) == 6:
        return True
    return False


def get_stock_name(ticker: str):
    """
    ì£¼ì‹ í‹°ì»¤ì— ëŒ€í•´ ì¢…ëª© ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    í•œêµ­ ì£¼ì‹ì´ë©´ pykrx, ì•„ë‹ˆë©´ yfinanceë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if is_korea_stock(ticker):
        try:
            from pykrx import stock
            ticker_name = stock.get_market_ticker_name(ticker)
            return ticker_name
        except Exception:
            return ""
    else:
        try:
            import yfinance as yf
            ticker_obj = yf.Ticker(ticker)
            info = ticker_obj.info
            # shortNameì´ ìˆìœ¼ë©´ ê·¸ê±¸, ì—†ìœ¼ë©´ longName, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ""
            return info.get("shortName") or info.get("longName") or ""
        except Exception:
            return ""


def get_naver_news(
    query: str,
    curr_date: str,
    look_back_days: int = 7,
    display: int = 10
) -> str:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê²€ìƒ‰ (ê¸°ì¡´ interface.py ìŠ¤íƒ€ì¼ì— ë§ì¶¤)
    
    Args:
        query: ê²€ìƒ‰ì–´
        curr_date: í˜„ì¬ ë‚ ì§œ (yyyy-mm-dd í˜•ì‹)
        look_back_days: ê²€ìƒ‰ ê¸°ê°„ (ê¸°ë³¸ 7ì¼)
        display: í‘œì‹œí•  ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
    
    Returns:
        str: í¬ë§·ëœ ë‰´ìŠ¤ ë¬¸ìì—´
    """
    
    print(f"[NaverNews] ğŸš€ ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜ ì‹œì‘ - ì¿¼ë¦¬: '{query}', ë‚ ì§œ: {curr_date}, ê¸°ê°„: {look_back_days}ì¼")
    
    # í•œêµ­ ì£¼ì‹ì˜ ê²½ìš° ì¢…ëª©ëª…ë„ í•¨ê»˜ ê²€ìƒ‰
    stock_name = get_stock_name(query)
    if stock_name:
        enhanced_query = f"{stock_name}"
        print(f"[NaverNews] ğŸ¢ í•œêµ­ ì£¼ì‹ ì¸ì‹ - '{query}' â†’ '{stock_name}' ë³€ê²½")
    else:
        enhanced_query = query
        print(f"[NaverNews] ğŸŒ ì¼ë°˜ ê²€ìƒ‰ì–´ë¡œ ì²˜ë¦¬")
    
    client = get_naver_search_client()
    
    # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
    news_results = client.search_news(
        query=enhanced_query,
        display=display,
        sort="date"  # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
    )
    
    if news_results.get("error"):
        print(f"[NaverNews] âŒ API ì—ëŸ¬: {news_results['error']}")
        return f"Naver News API Error: {news_results['error']}"
    
    news_items = news_results.get("news", [])
    
    if not news_items:
        print(f"[NaverNews] âš ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return ""
    
    print(f"[NaverNews] ğŸ“° {len(news_items)}ê°œ ë‰´ìŠ¤ ì•„ì´í…œ í™•ë³´")
    
    # ë‚ ì§œ í•„í„°ë§ (look_back_days ê¸°ê°„ ë‚´)
    start_date = datetime.strptime(curr_date, "%Y-%m-%d")
    before = start_date - timedelta(days=look_back_days)
    before_str = before.strftime("%Y-%m-%d")
    
    print(f"[NaverNews] ğŸ“… ë‚ ì§œ í•„í„°ë§ - {before_str} ~ {curr_date}")
    
    filtered_items = []
    for item in news_items:
        try:
            item_date = datetime.strptime(item["published_date"], "%Y-%m-%d")
            if item_date >= before:
                filtered_items.append(item)
        except:
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨
            filtered_items.append(item)
    
    print(f"[NaverNews] ğŸ“Š ë‚ ì§œ í•„í„°ë§ ê²°ê³¼ - {len(news_items)}ê°œ â†’ {len(filtered_items)}ê°œ")
    
    # ê²°ê³¼ í¬ë§¤íŒ… (ê¸°ì¡´ Google News ìŠ¤íƒ€ì¼ê³¼ ìœ ì‚¬í•˜ê²Œ)
    print(f"[NaverNews] ğŸ“ ê²°ê³¼ í¬ë§¤íŒ… ì‹œì‘")
    news_str = ""
    for i, news in enumerate(filtered_items):
        news_str += (
            f"### {news['title']} (source: {news['source']}) \n\n{news['summary']}\n\n"
        )
        if i == 0:  # ì²« ë²ˆì§¸ ë‰´ìŠ¤ë§Œ ë¡œê¹…
            print(f"[NaverNews] ğŸ“° ì²« ë²ˆì§¸ ë‰´ìŠ¤: {news['title'][:50]}...")
    
    if len(filtered_items) == 0:
        print(f"[NaverNews] âŒ ìµœì¢… ê²°ê³¼ ì—†ìŒ")
        return ""
    
    result = f"## {query} Naver News, from {before_str} to {curr_date}:\n\n{news_str}"
    print(f"[NaverNews] âœ… ìµœì¢… ê²°ê³¼ ìƒì„± ì™„ë£Œ - {len(result)}ì, {len(filtered_items)}ê°œ ë‰´ìŠ¤")
    return result